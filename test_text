import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import lit, unix_timestamp, explode_outer, col, concat_ws, to_json, udf, expr
from pyspark.sql.types import StringType
from pyspark.sql import SparkSession
import boto3
from datetime import datetime, timedelta
import requests
import logging
import zipfile
import io
import pytz
from elasticsearch import Elasticsearch, RequestsHttpConnection, helpers, ElasticsearchException
from requests_aws4auth import AWS4Auth
import psycopg2
import re
import json
from functools import partial
import uuid



sys.argv += ['--JOB_NAME', 'cde_si_job_ndc_codes_test', '--kw_s3bucket', 'my_bucket', '--es_endpoint', 'https://my_endpoint.us-east-1.es.amazonaws.com']

REGION = 'us-east-1'
NY_TZ = pytz.timezone('US/Eastern')


def setup_logging(job_name):
    logging.basicConfig(format='%(asctime)s %(levelname)s %(name)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    logger = logging.getLogger(job_name)
    logger.setLevel(logging.INFO)
    return logger


def get_aws_auth():
    credentials = boto3.Session().get_credentials()
    return AWS4Auth(credentials.access_key, credentials.secret_key, REGION, 'es', session_token=credentials.token)


def get_es_client(es_endpoint):
    awsauth = get_aws_auth()
    return Elasticsearch(
        [es_endpoint],
        http_auth=awsauth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection
    )


def format_records_for_elastic(recordList, index: str):
    for recordRow in recordList:
        record = recordRow.asDict()
        # print(f"Formatting record {record.get('id')} for insert")
        yield {
            "_index": index,
            "_id": record.get("id"),
            "_op_type": "index",
            "_source": record
        }


def bulk_upload(es_endpoint, index):
    def bulk_upload_partitioned(partitioned_records):
        failure_ids = []
        es = get_es_client(es_endpoint)
        try:
            result = helpers.bulk(
                es,
                format_records_for_elastic(partitioned_records, index),
                stats_only=False,
                raise_on_error=False,
                raise_on_exception=False,
                max_retries=2,
                initial_backoff=1,
                chunk_size=100
            )
            print(result)
            print(f"Number of records successfully added to ES index {index}: {result[0]}")
            print(f"Number of records failed to added Elasticsearch: {result[1]}")
            failures = result[1]
            if failures:
                failure_ids = [failed_record.get('index').get('id') for failed_record in failures if failed_record.get('index')]
        except ElasticsearchException as ex:
            print("bulk API error")
            print(ex, exc_info=True)

        print(f"List of Id's that failed to update in ES {failure_ids}")
        if failure_ids:
            yield failure_ids
    return bulk_upload_partitioned


def manage_es_alias(es, es_index, es_alias):
    if es.indices.exists_alias(name=es_alias):
        current_indices = list(es.indices.get_alias(name=es_alias).keys())
        old_index = current_indices[0] if current_indices else None
    else:
        old_index = None
    if old_index:
        es.indices.update_aliases({
            "actions": [
                {"remove": {"index": old_index, "alias": es_alias}},
                {"add": {"index": es_index, "alias": es_alias}}
            ]
        })
        es.indices.delete(index=old_index)
    else:
        es.indices.put_alias(index=es_index, name=es_alias)


def main():
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'kw_s3bucket', 'src_url', 'es_endpoint'])

    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)

    logger = setup_logging(args['JOB_NAME'])
    s3 = boto3.client('s3')

    s3_json_path = 's3://hf-dev-cdp-kitewheel/drug_ndc_codes/latest/cleaned_drug-ndc-0001-of-0001.json'
    spark_df = spark.read.option("multiLine", True).option("mode", "PERMISSIVE").json(s3_json_path)
    src_cnt = spark_df.count()
    logger.info(f"Count of records in source json file: {src_cnt}")

    es_index = 'drug_ndc_codes_es_' + datetime.now(NY_TZ).strftime('%Y%m%d%H%M%S')
    logger.info(f"Start: Load documents to ES index: {es_index}")
    failures = spark_df.rdd.mapPartitions(bulk_upload(args['es_endpoint'], es_index))
    logger.info(f'failues: {failures.collect()}')
    logger.info(f"End: Load documents to ES index: {es_index}")

    logger.info("Start: Switch index for alias")
    es = get_es_client(args['es_endpoint'])
    manage_es_alias(es, es_index, 'drug_ndc_codes_es')
    logger.info("End: Switch index for alias")
    job.commit()


if __name__ == "__main__":
    main()
