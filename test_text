import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import current_date
from pyspark.sql import SparkSession
import boto3
from datetime import datetime
import requests
import zipfile
import io
from elasticsearch import Elasticsearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

def get_aws_auth():
    credentials = boto3.Session().get_credentials()
    return AWS4Auth(credentials.access_key, credentials.secret_key, args['AWS_REGION'], 'es', session_token=credentials.token)

def get_es_client():
    awsauth = get_aws_auth()
    return Elasticsearch(
        hosts=[{'host': 'your-es-domain', 'port': 443}],
        http_auth=awsauth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection
    )

def download_and_extract_zip(url, bucket_name, s3_folder):
    response = requests.get(url)
    response.raise_for_status()
    with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip:
        for file_name in the_zip.namelist():
            with the_zip.open(file_name) as file:
                cleaned_s3_key = f'{s3_folder}/cleaned_{file_name}'
                stream_clean_json(file, bucket_name, cleaned_s3_key)
                return f's3://{bucket_name}/{cleaned_s3_key}'

def stream_clean_json(file_stream, bucket_name, s3_key):
    str_io = io.StringIO(file_stream.read().decode('utf-8'))
    output_io = io.StringIO()
    capture = False
    buffer = []
    for line in str_io:
        if '"results": [' in line:
            capture = True
        if capture:
            buffer.append(line)
    buffer.pop()  # Remove the last line
    output_io.writelines(buffer)
    output_io.seek(0)
    s3 = boto3.client('s3')
    s3.upload_fileobj(io.BytesIO(output_io.getvalue().encode()), bucket_name, s3_key)
    output_io.close()
    str_io.close()

def manage_es_alias(es, es_index, es_alias):
    if es.indices.exists_alias(name=es_alias):
        current_indices = list(es.indices.get_alias(name=es_alias).keys())
        old_index = current_indices[0] if current_indices else None
    else:
        old_index = None
    if old_index:
        es.indices.update_aliases({
            "actions": [
                {"remove": {"index": old_index, "alias": es_alias}},
                {"add": {"index": es_index, "alias": es_alias}}
            ]
        })
        es.indices.delete(index=old_index)
    else:
        es.indices.put_alias(index=es_index, name=es_alias)

def load_data_to_es(spark_df, es_index):
    es = get_es_client()
    spark_df.write.format("org.elasticsearch.spark.sql").option("es.resource", f'{es_index}/_doc').option("es.nodes", 'your-es-domain').option("es.port", '443').option("es.nodes.wan.only", "true").save()
    manage_es_alias(es, es_index, 'ndc_data')

def main():
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'AWS_REGION'])
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = SparkSession.builder.getOrCreate()
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)
    
    src_url = "https://api.fda.gov/download.json"
    ndc_url = requests.get(src_url).json()['results']['drug']['ndc']['partitions'][0]['file']
    
    s3_json_path = download_and_extract_zip(ndc_url, 'mys3bucket', 'ndc_test/to/json-files')
    if s3_json_path:
        spark_df = spark.read.option("multiLine", True).option("mode", "PERMISSIVE").json(s3_json_path)
        spark_df = spark_df.withColumn("ingest_date", current_date())
        es_index = 'ndc_data_' + datetime.now().strftime('%Y%m%d')
        load_data_to_es(spark_df, es_index)
        print("Data loaded into Elasticsearch successfully.")
    job.commit()

if __name__ == "__main__":
    main()
