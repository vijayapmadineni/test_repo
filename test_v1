import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import lit, unix_timestamp
from pyspark.sql.types import *
import boto3
import psycopg2
from datetime import datetime, timedelta
import pytz
import logging
import re
import requests
import zipfile
import io
import json

args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
job_name=args['JOB_NAME']

def upload_to_s3(bucket_name, s3_key, file_data):
    try:
        s3 = boto3.client('s3')
        s3.upload_fileobj(file_data, bucket_name, s3_key)
    except NoCredentialsError:
        print("Credentials not available")
    except Exception as e:
        print(str(e))
        

def download_and_extract_zip(url, bucket_name, s3_folder):
    try:
        response = requests.get(url)
        response.raise_for_status()

        with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip:
            for file_name in the_zip.namelist():
                print(f'Extracting {file_name}')
                # Extract file content
                with the_zip.open(file_name) as file:
                    s3_key = f'{s3_folder}/{file_name}'
                    upload_to_s3(bucket_name, s3_key, file)
                    return f's3://{bucket_name}/{s3_key}'
    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}")
    except Exception as err:
        print(f"Other error occurred: {err}")


# Define S3 parameters
bucket_name = 'hf-dev-cdp-kitewheel'
s3_folder = 'ndc_test/to/json-files'

# Fetching the JSON data from the URL
src_url = "https://api.fda.gov/download.json"
response = requests.get(src_url)
data = response.json()

# Extract the NDC URL
ndc_url = data['results']['drug']['ndc']['partitions'][0]['file']

# Continue processing as needed
print(ndc_url)

# Download and extract the NDC data, and get the S3 path of the JSON
s3_json_path = download_and_extract_zip(ndc_url, bucket_name, s3_folder)

# Define the schema for active ingredients
active_ingredients_schema = StructType([
    StructField("name", StringType(), True),
    StructField("strength", StringType(), True),
])

# Define the schema for packaging
packaging_schema = StructType([
    StructField("package_ndc", StringType(), True),
    StructField("description", StringType(), True),
    StructField("marketing_start_date", DateType(), True),
    StructField("marketing_end_date", DateType(), True),
    StructField("sample", StringType(), True),
])

# Define the schema for the openfda section
openfda_schema = StructType([
    StructField("is_original_packager", StringType(), True),
    StructField("manufacturer_name", ArrayType(StringType()), True),
    StructField("nui", StringType(), True),
    StructField("pharm_class_cs", StringType(), True),
    StructField("pharm_class_epc", StringType(), True),
    StructField("pharm_class_moa", StringType(), True),
    StructField("pharm_class_pe", StringType(), True),
    StructField("rxcui", ArrayType(StringType()), True),
    StructField("spl_set_id", StringType(), True),
    StructField("unii", StringType(), True),
    StructField("upc", StringType(), True),
])

# Define the complete schema for the NDC dataset
ndc_schema = StructType([
    StructField("product_id", StringType(), True),
    StructField("product_ndc", StringType(), True),
    StructField("spl_id", StringType(), True),
    StructField("product_type", StringType(), True),
    StructField("finished", StringType(), True),
    StructField("brand_name", StringType(), True),
    StructField("brand_name_base", StringType(), True),
    StructField("brand_name_suffix", StringType(), True),
    StructField("generic_name", StringType(), True),
    StructField("dosage_form", StringType(), True),
    StructField("route", StringType(), True),
    StructField("marketing_start_date", DateType(), True),
    StructField("marketing_end_date", DateType(), True),
    StructField("marketing_category", StringType(), True),
    StructField("application_number", StringType(), True),
    StructField("pharm_class", StringType(), True),
    StructField("dea_schedule", StringType(), True),
    StructField("listing_expiration_date", DateType(), True),
    StructField("active_ingredients", ArrayType(active_ingredients_schema), True),
    StructField("packaging", ArrayType(packaging_schema), True),
    StructField("openfda", openfda_schema, True),
])

# Convert to Spark DataFrame
if s3_json_path:
    # spark_df = spark.read.schema(ndc_schema).json(s3_json_path)
    spark_df = spark.read.option("multiLine", True).option("mode", "PERMISSIVE").json(s3_json_path)

    # Show schema and data
    spark_df.printSchema()
    spark_df.show()
    print('NDC data was read successfully.')

# Stop Spark Context and commit job
job.commit()
