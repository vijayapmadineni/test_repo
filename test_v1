I have below job, please implement above function stream_clean_json in this once file is downloaded it needs to be cleaned and upload_to_s3


import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import lit, unix_timestamp
from pyspark.sql.types import *
import boto3
import psycopg2
from datetime import datetime, timedelta
import pytz
import logging
import re
import requests
import zipfile
import io
import json

args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
job_name=args['JOB_NAME']

def upload_to_s3(bucket_name, s3_key, file_data):
    try:
        s3 = boto3.client('s3')
        s3.upload_fileobj(file_data, bucket_name, s3_key)
    except NoCredentialsError:
        print("Credentials not available")
    except Exception as e:
        print(str(e))
        

def download_and_extract_zip(url, bucket_name, s3_folder):
    try:
        response = requests.get(url)
        response.raise_for_status()

        with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip:
            for file_name in the_zip.namelist():
                print(f'Extracting {file_name}')
                # Extract file content
                with the_zip.open(file_name) as file:
                    cleaned_s3_key = f'{s3_folder}/cleaned_{file_name}'
                    stream_clean_json(file, bucket_name, cleaned_s3_key)
                    return f's3://{bucket_name}/{cleaned_s3_key}'
    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}")
    except Exception as err:
        print(f"Other error occurred: {err}")


import io
import boto3

def stream_clean_json(file_stream, bucket_name, s3_key):
    # Convert byte stream to StringIO for text manipulation
    str_io = io.StringIO(file_stream.read().decode('utf-8'))
    output_io = io.StringIO()
    
    capture = False
    buffer = []  # to handle the removal of the last line
    for line in str_io:
        if '"results": [' in line:
            line = line.replace('"results": [', '[')  # Start capturing after replacing the line
            capture = True
        if capture:
            buffer.append(line)
    
    # Remove the last line from buffer before writing to output
    if buffer:
        buffer.pop()  # Remove the last line
    
    # Write the modified content to output_io
    for line in buffer:
        output_io.write(line)
    
    # Move back to the start of the StringIO buffer
    output_io.seek(0)
    
    # Upload the cleaned data to S3
    s3 = boto3.client('s3')
    s3.upload_fileobj(io.BytesIO(output_io.getvalue().encode()), bucket_name, s3_key)
    output_io.close()
    str_io.close()
    print(f"Cleaned file uploaded to s3://{bucket_name}/{s3_key}")

# Define S3 parameters
bucket_name = 'hf-dev-cdp-kitewheel'
s3_folder = 'ndc_test/to/json-files'

# Fetching the JSON data from the URL
src_url = "https://api.fda.gov/download.json"
response = requests.get(src_url)
data = response.json()

# Extract the NDC URL
ndc_url = data['results']['drug']['ndc']['partitions'][0]['file']

# Continue processing as needed
print(ndc_url)

# Download and extract the NDC data, and get the S3 path of the JSON
s3_json_path = download_and_extract_zip(ndc_url, bucket_name, s3_folder)

# Define the schema for active ingredients
# Download, clean, and extract the NDC data, and get the S3 path of the cleaned JSON
s3_json_path = download_and_extract_zip(ndc_url, bucket_name, s3_folder)

# Continue with your data processing as needed
if s3_json_path:
    spark_df = spark.read.option("multiLine", True).option("mode", "PERMISSIVE").json(s3_json_path)
    spark_df.printSchema()
    spark_df.show()
    print('NDC data was read successfully.')


# # Convert to Spark DataFrame
# if s3_json_path:
#     # spark_df = spark.read.schema(ndc_schema).json(s3_json_path)
#     spark_df = spark.read.option("multiLine", True).option("mode", "PERMISSIVE").json(s3_json_path)

#     # Show schema and data
#     spark_df.printSchema()
#     spark_df.show()
#     print('NDC data was read successfully.')

# # Stop Spark Context and commit job
# job.commit()
