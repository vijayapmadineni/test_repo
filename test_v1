import boto3
from botocore.exceptions import NoCredentialsError

def upload_to_s3(bucket_name, s3_key, file_data):
    try:
        s3 = boto3.client('s3')
        s3.upload_fileobj(file_data, bucket_name, s3_key)
    except NoCredentialsError:
        print("Credentials not available")
    except Exception as e:
        print(str(e))

def download_and_extract_zip(url, bucket_name, s3_folder):
    try:
        response = requests.get(url)
        response.raise_for_status()

        with zipfile.ZipFile(io.BytesIO(response.content)) as the_zip:
            for file_name in the_zip.namelist():
                print(f'Extracting {file_name}')
                # Extract file content
                with the_zip.open(file_name) as file:
                    s3_key = f'{s3_folder}/{file_name}'
                    upload_to_s3(bucket_name, s3_key, file)
                    return f's3://{bucket_name}/{s3_key}'
    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}")
    except Exception as err:
        print(f"Other error occurred: {err}")


# Define S3 parameters
bucket_name = 'your-bucket-name'
s3_folder = 'path/to/json-files'

# Download and extract the NDC data, and get the S3 path of the JSON
s3_json_path = download_and_extract_zip(ndc_url, bucket_name, s3_folder)

from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DateType

# Define the schema for active ingredients
active_ingredients_schema = StructType([
    StructField("name", StringType(), True),
    StructField("strength", StringType(), True),
])

# Define the schema for packaging
packaging_schema = StructType([
    StructField("package_ndc", StringType(), True),
    StructField("description", StringType(), True),
    StructField("marketing_start_date", DateType(), True),
    StructField("marketing_end_date", DateType(), True),
    StructField("sample", StringType(), True),
])

# Define the schema for the openfda section
openfda_schema = StructType([
    StructField("is_original_packager", StringType(), True),
    StructField("manufacturer_name", ArrayType(StringType()), True),
    StructField("nui", StringType(), True),
    StructField("pharm_class_cs", StringType(), True),
    StructField("pharm_class_epc", StringType(), True),
    StructField("pharm_class_moa", StringType(), True),
    StructField("pharm_class_pe", StringType(), True),
    StructField("rxcui", ArrayType(StringType()), True),
    StructField("spl_set_id", StringType(), True),
    StructField("unii", StringType(), True),
    StructField("upc", StringType(), True),
])

# Define the complete schema for the NDC dataset
ndc_schema = StructType([
    StructField("product_id", StringType(), True),
    StructField("product_ndc", StringType(), True),
    StructField("spl_id", StringType(), True),
    StructField("product_type", StringType(), True),
    StructField("finished", StringType(), True),
    StructField("brand_name", StringType(), True),
    StructField("brand_name_base", StringType(), True),
    StructField("brand_name_suffix", StringType(), True),
    StructField("generic_name", StringType(), True),
    StructField("dosage_form", StringType(), True),
    StructField("route", StringType(), True),
    StructField("marketing_start_date", DateType(), True),
    StructField("marketing_end_date", DateType(), True),
    StructField("marketing_category", StringType(), True),
    StructField("application_number", StringType(), True),
    StructField("pharm_class", StringType(), True),
    StructField("dea_schedule", StringType(), True),
    StructField("listing_expiration_date", DateType(), True),
    StructField("active_ingredients", ArrayType(active_ingredients_schema), True),
    StructField("packaging", ArrayType(packaging_schema), True),
    StructField("openfda", openfda_schema, True),
])

# Example of using this schema to read JSON data
# df = spark.read.schema(ndc_schema).json("path_to_ndc_json_file.json")
# df.show()

# Convert to Spark DataFrame
if s3_json_path:
    spark_df = spark.read.json(s3_json_path)

    # Show schema and data
    spark_df.printSchema()
    spark_df.show()
    print('NDC data was read successfully.')

# Stop Spark Context and commit job
job.commit()
